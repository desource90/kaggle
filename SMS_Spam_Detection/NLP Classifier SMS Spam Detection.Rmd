---
title: "NLP Classifier for SMS messages"
author: "Desource90"
date: "February 8, 2017"
output: 
  html_document: 
    keep_md: yes
    toc: yes
---

```{r setup, include=FALSE}
library(needs)
needs(formattable, # visuals
      corrplot, # visuals
      wordcloud, # visuals
      DT, # visuals
      tidyverse, # data cleaning
      stringr, # string manipulation
      randomForest, # classification
      tm, # Text mining
      ROSE, # Dealing w/ imbalanced class
      magrittr, # %<>% piping
      caret # confusion matrix)
      )
```

# Introduction
I wanted to practice some NLP libraries and there's no better way than the classic spam versus ham problem. The goal of this project will be to build a classifier to predict SMS messages as spam or not and 'productionalize' using shiny.

## Load & check data
```{r}
sms = suppressMessages(read_delim("SMSSpamCollection", delim = "\t", col_names = c("label", "message"), escape_double = FALSE))
sms %>% 
  head(5e2) %>% 
  formattable(align = "l") %>% 
  as.datatable(rownames = FALSE, options = list(scrollX = TRUE))
```

# Feature Engineering

## Basic Assumptions about Spam

The stereotypical spam message is the lengthy marketing message that tries to grab our attention and usually involves some sort of prize winning, so we can add some extra variables based assumptions about how spam is usually presented...

* Usually longer length
* Excessive use of caps
* More numbers than usual due to prize winning mentions
* Lots of !!!! to grab our attention

Good to see our assumption are correct and the means for all the variables we added are higher for spam class than non-spam
```{r, echo=FALSE}
sms %<>% mutate(msg_length = str_length(message),
                pct_caps = str_count(message, "[A-Z]") / msg_length,
                pct_digits = str_count(message,"\\d") / msg_length,
                num_exclamations = str_count(message, "!"),
                num_caps = str_count(message, "[A-Z]"),
                num_digits = str_count(message,"\\d"),
                # consecutive_digits = str_count(message,"\\d+"),
                numeric_label = as.numeric(as.factor(label)))

sms %>% 
  select(-message) %>% 
  group_by(label) %>% 
  summarise_all(function(x) round(mean(x),3)) %>% 
  formattable( align = "l")
```

### Variable Correlation

```{r}
corrplot(cor(sms %>% select(-label,-message)), type = "lower", method="shade",tl.col="black", tl.srt=45, addCoef.col="black")
sms$numeric_label = NULL
```

# Spam & Non-Spam Word Clouds

Here, we'll use the text mining package to help us tokenize the text and represent a SMS message as columns of term frequencies contained in the entire corpus. Since many words only occur infrequently and not in many documents, we'll also remove those terms to reduce the sparsity and work with less columns of data. The tm package helps us do the heavy lifting with regards to that.
```{r}
# Helper function
create_corpus <- function(x) {
  # Creat corpus
  result_corpus <- Corpus(VectorSource(x)) %>% 
    # run various cleaning functions on corpus
    tm_map(tolower, lazy = T) %>% 
    tm_map(PlainTextDocument) %>% 
    tm_map(removePunctuation) %>% 
    tm_map(removeWords, c(stopwords("english"))) %>% 
    tm_map(stripWhitespace) %>% 
    tm_map(stemDocument, lazy = T)
  return(result_corpus)
}

create_term_frequency_counts <- function(dtm) {
  m <- as.matrix(t(dtm))
	v <- sort(rowSums(m), decreasing=TRUE)
	d <- data.frame(word = names(v), freq=v, stringsAsFactors = FALSE)
	return (d)
}


corpus_ham = create_corpus(sms$message[sms$label=="ham"])
corpus_spam = create_corpus(sms$message[sms$label=="spam"])

dtm_ham <- DocumentTermMatrix(corpus_ham) %>% 
  removeSparseTerms(0.995)
dtm_spam <- DocumentTermMatrix(corpus_spam) %>% 
  removeSparseTerms(0.99)

wordfreq_ham = create_term_frequency_counts(dtm_ham)
wordfreq_spam = create_term_frequency_counts(dtm_spam)
word_freq = full_join(wordfreq_ham, wordfreq_spam, by = c("word"), suffix =  c("_ham", "_spam"))

par(mfrow=c(1, 2))
# Add word cloud for ham in green
wordcloud(corpus_ham, min.freq = 10, max.words = 100, scale=c(3, .2), random.order = FALSE, use.r.layout=FALSE, colors = c("springgreen1","springgreen2","springgreen3","springgreen4"))
# Add word cloud for spam in red
wordcloud(corpus_spam, min.freq = 3, max.words = 100, scale=c(5, .2), random.order = FALSE, use.r.layout=FALSE, colors = c("indianred1","indianred2","indianred3","indianred"))
```

Basic message of SMS spam seems to boils down to !!reply back NOW to get your free stuff!!

# Prediction

We'll create a document term matrix but only include the most frequent terms we found for the spam and ham groups previously. The rationale for splitting into the two classes before removing sparse terms was to avoid potentially excluding terms significant in the spam class. 

```{r}
corpus = create_corpus(sms$message)
dtm = DocumentTermMatrix(corpus)
dtm = as.data.frame(as.matrix(dtm)) %>% 
  select(one_of(word_freq$word))
  
colnames(dtm) <- make.names(colnames(dtm))

dtm = suppressWarnings(cbind(dtm,sms %>% select(-message)) %>% 
  mutate(label = as.factor(label)))

dtm %>% count(label) %>% 
  formattable(align = "l")
  
# Split data into test and train
set.seed(12)
n = nrow(dtm)
idx <- sample(n, n * .75)
train = dtm[idx,]
test = dtm[-idx,]
```

We also have very imbalanced class labels. Since our data isn't so large that run-time is a concern, we'll handle that by with sampling with replacement from the under-represented spam class. Using some info taken from: 
https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/ 

## Dealing with Class Imbalance - Oversampling

```{r}
#over sampling
# Make 50/50
N = train %>% 
  filter(label=="ham") %>% 
  nrow() * 2
data_balanced_over <- ovun.sample(label ~ ., data = train, method = "over",N = N)$data
```

## RandomForest

```{r}
# Set seed
set.seed(661)

# Build model
rf_model <- randomForest(x = data_balanced_over %>% select(-label), y = data_balanced_over$label, importance = TRUE, ntree = 100)

# Show model error
plot(rf_model)
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
```

Not too bad, wonder how the importance of our variables included look right now
```{r}
# Get importance, include top 15 since we have too many variables
importance <- importance(rf_model)
var_importance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ , 'MeanDecreaseGini'], 2)) %>% 
  arrange(desc(Importance)) %>% 
  head(15)

# plot it
ggplot(var_importance, 
       aes(x = reorder(Variables, Importance), y = Importance)) +
  geom_bar(stat='identity') +
  labs(x = 'Variables', 
       title = 'Relative Variable Importance') +
  coord_flip()
```

Wow, I'm a little surprised to see that the most important variables were just the ones we derived based on our assumptions in the beginning. A big reason might be because the actual tokenized features were rather sparse. It's also interesting to see `call` up so high, since that word also showed up for the ham class in the word cloud.

## Model Prediction and Performance on Test
```{r}
predict.rf = predict(rf_model, test)
confusionMatrix(predict.rf, test$label)
```

# Using a Simpler Model
Given that the most important features were what we derived upfront, let's see how well our model performs without using any of the term frequency features.

```{r}
set.seed(672)
rf_simple = randomForest(data_balanced_over %>% select(num_digits, pct_digits, msg_length, num_caps, pct_caps, num_exclamations), data_balanced_over$label, importance = TRUE, ntree = 100)
predict.rf_simple = predict(rf_simple, test)
confusionMatrix(predict.rf_simple, test$label)
```


# Conclusion

Very surprising that our simple model that doesn't use any bag-of-words features performs just as well as the previous model. It also takes a lot less data preparation and less time to train. To improve our predictions, we could probably add additional features such as the # of currency symbols in the message, as well as modify the loss function so that non-spam that's incorrectly classified as spam to be more heavily penalized.

